# -*- coding: utf-8 -*-
"""Churn Prediction Project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KBA-ZKPDetaoZURlCCAf6ezel7662hC2

# Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries
import pandas as pd # Import Pandas library
import numpy as np # Import NumPy library

import seaborn as sns # Import Seaborn
from matplotlib import pyplot as pyplot # Import Plotting tools
# %matplotlib inline

"""# **Section 3.1: Churn Prediction Project**

The entire code of this project is available in this [Jupyter Notebook](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-03-churn-prediction/03-churn.ipynb).

Consider a phone company with some customers; let's call this company Telco. As is the case with customers, they may or may not remain loyal to Telco. Customers may very well be satisfied with the service they're receiving at Telco; however, as is the nature of customers, they are always in search of the best deals.

Suppose there is a competitor who emerges, let's call them Telco2 (for the sake of argument, avoid the copyright lawsuit). Some customers may consider leaving to Telco2. The objective for this project is to predict which customers would churn, or leave Telco to go receive services from Telco2 (the competitor).

We are going to identify these customers, and assign them a score between 0 and 1, likely based on probability. This score will be used to determine the likelihood the customer will leave.

The purpose of making these assessments is prevent customers from churning; this means businesses will act on data to find ways to retain the customers (i.e. special promotion, discount, etc.).

The way this will be approached is through **binary classification**.

Unlike the previous chapter, the y_i (the prediction) will be a probability that the customer will, leave; it will be a specific customer, i.

y_i exists in the domain [0, 1], where 1 is a positive conclusion (that is the customer did churn; 0 is that the customer did not churn.

This project is going to be reminiscent of the spam/not spam examples.

The information about the customers is X.

[Dataset from Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

[Raw Dataset Link](https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv)

# **Section 3.2: Data Preparation**

[Section 3.2 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/02-data-preparation.md)

*   Download the data, read it with pandas
*   Look at the data
*   Make column names and values look uniform
*   Check if all the columns read correctly
*   Check if the churn variable needs any preparation

This session covered data obtention and some procedures of data preparation.

Commands, functions, and methods:

*   `!wget` - Linux shell command for downloading data
*   `pd.read.csv()` - read csv files
*   `df.head()` - take a look of the dataframe
*   `df.head().T` - take a look of the transposed dataframe
*   `df.columns` - retrieve column names of a dataframe
*   `df.columns.str.lower()` - lowercase all the letters
*   `df.columns.str.replace(' ', '_')` - replace the space separator
*   `df.dtypes` - retrieve data types of all series
*   `df.index` - retrive indices of a dataframe
*   `pd.to_numeric()` - convert a series values to numerical values. The `errors=coerce` argument allows making the transformation despite some encountered errors.
*   `df.fillna()` - replace NAs with some value
*   `(df.x == "yes").astype(int)` - convert x series of yes-no values to numerical values.

## Get the Data
"""

# Input the data from the URL.
data = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'

!wget $data -O data-week-3.csv 
# Save in  data week 3 file.
# The ! means we are executing a shell command.
# The linux command is specifically what executes the retrieval.
# The $ is how you refer to variables.

"""## Read the Data Using Pandas"""

df = pd.read_csv('data-week-3.csv') # Load the data using the read_csv command; 
# store as a dataframe. Make sure to input the correct file name in the argument

df.shape # Display dataframe dimensions.

"""The dataframe has 7,043 rows and 21 columns."""

df.head() # Display the first five rows of the dataframe.

"""Note that there are 21 columns. Note that between the OnlineSecurity (column 10) and the DeviceProtection columns, there is a set of ellipses that indicate the number of columns has been compressed.

Transpose the Dataframe to View All Columns
"""

df.head().T # Transpose the dataframe

"""Customer number will be along the top beginning with Customer #0 up to Customer #7042."""

len(df) # Display dataframe length

"""## Normalize the Dataframe"""

df.head()

# Format the columns string elements to be all lowercase and replace spaces
# with underscores; df.columns will replace the columns of the original
# dateframe
df.columns = df.columns.str.lower().str.replace(' ', '_')

# Displays a list of the newly formatted columns
categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)

# Compose a for loop to replace the space each element, c of the dataframe with
# an underscore.
for c in categorical_columns:
  df[c] = df[c].str.lower().str.replace(' ', '_')

df.head().T # Display the transposed dataframe with formatted elements.

"""Next, look at the data types."""

df.dtypes # Display the datatypes for each column in the dataframe.

"""Most of the datatypes are strings. A couple of the columns have integers and one has floating numbers.

The totalcharges column is listed as having 'object' datatypes when you would expect either a float64 or int64.
"""

df.totalcharges # Display the totalcharges column.

"""When we attempt to convert the characters in the totalcharges column to strings, we get an error.

Commented out
"""

# pd.to_numeric(df.totalcharges) # Attempt to change the datatypes from strings
# to numeric. The error indicates there are nonnumerical elements in this
# column.

tc = pd.to_numeric(df.totalcharges, errors = 'coerce') # Converts elements to 
# numeric datatypes; and the 'coerce' input forces the conversion despite not
# being eligible for conversion.

df[tc.isnull()][['customerid', 'totalcharges']] # Display the null values
# from customerid and totalcharges columns

df.totalcharges = pd.to_numeric(tc) # Replace
# the totalcharges column with tc.

df.totalcharges = df.totalcharges.fillna(0) # Fill the null values in the
# totalcharges column with 0.

"""Ideally, you do not want to replace null values with zero; but in a practice scenario, we can replace the null values with zero."""

df.churn.head() # Display first five rows of churn column

(df.churn == 'yes').head() # Displays the boolean; yes is true, no is false

"""We want to replace the boolean value with a number.

## Convert the Churn Column into a Binary Column
"""

df.churn = (df.churn == 'yes').astype(int) # Replaces the boolean values 
# with an integer; and replaces the churn column with new integer values.
df.churn # Display the first five rows of the churn column

df.head().T

len(df.churn)

"""# **Section 3.3: Setting Up the Validation Framework**

[Section 3.3 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/03-validation.md)

*   Perform the train/validation/test split with Scikit-Learn

Recall the dataframe distribution breakdown diagram: training data(60%), validation data (20%), and test data (20%).

This will be done with Scikit-Learn.

Scikit-Learn is a python library that has implementation in machine learning and data science. It has features that execute a train-test-split of data.

## Import Train-Test-Split from Scikit-Learn
"""

from sklearn.model_selection import train_test_split

"""## `train_test_split?` 

The question mark displays a help window indicating the distribution size for each dataset. Commented out to avoid error.

## Split the Dataframe into `df_full_train` and `df_test`

Since the `train_test_split` command only splits the data into training and test, where the test size is 20%, we have the training data left at 80%, called `full_training`. We need to split the `full_training` dataset into training and validation.
"""

df_full_train, df_test = train_test_split(df, test_size = 0.2, random_state = 1)
# Splits the dataframe to have a test size of 20%, with random seed 1.
# This command splits the data into two parts: training and testing.

len(df_full_train), len(df_test) # Display the size of each dataset.

"""## Split `df_full_train` into `df_train` and `df_val`"""

df_train, df_val = train_test_split(df_full_train, test_size = 0.25, random_state = 1)
# Splits 25% of df_val from df_full_train with random seed 1.
# 20% of the 80% training data is 25%, hence why the validation data will be
# 25% of df_full_train.

len(df_train), len(df_val), len(df_test) # Display the size of each dataset.

"""## Unshuffles the Index in Each Dataset"""

df_train.reset_index(drop = True) # Unshuffles the index for df_train
df_val.reset_index(drop = True) # Unshuffles the index for df_val
df_val.reset_index(drop = True) # Unshuffles the index for df_test

"""Next we need to get our y variables."""

y_train = df_train.churn.values # Gets the churn values from the training data;
# stored as y_train
y_val = df_val.churn.values # Gets the churn values from the validation data;
# stored as y_val
y_test = df_test.churn.values # Gets the churn values from the validation data;
# stored as y_test

"""## Delete Churn Column from Dataframes"""

del df_train['churn'] # Deletes the churn variable from the training data
del df_val['churn'] # Deletes the churn variable from the validation data
del df_test['churn'] # Deletes the churn variable from the test data

"""Note that the churn variable was not deleted from `df_full_train`. This will be addressed in Section 3.4.

# **Section 3.4: Exploratory Data Analysis (EDA)**

[Section 3.4 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/04-eda.md)

*   Check missing values
*   Look at the target variable (churn)
*   Look at numerical and categorical variables

Functions and methods:

*   `df.isnull().sum()` - returns the number of null values in the dataframe.
*   `df.x.value_counts()` returns the number of values for each category in x series. The `normalize=True` argument retrieves the percentage of each category. In this project, the mean of churn is equal to the churn rate obtained with the value_counts method.
*   `round(x, y)` - round an x number with y decimal places
*   `df[x].nunique()` - returns the number of unique values in x series

## Begin with the `full_train` dataset.
"""

df_full_train = df_full_train.reset_index(drop = True) # Restore default indexes

df_full_train # Display training dataset

"""## Check for missing values."""

df_full_train.isnull().sum() # Displays the number of null values in the
# training dataframe.

"""There are no missing values; so, we don't need to perform any additional data preparation."""

df_full_train.churn.value_counts() # Displays the count for 0 and 1 in the churn
# column.

"""We are seeing the distribution for how many users are churning and how many are being retained. In this case, out of the 5,634 customers in the dataset, Telcom is retaining approximately 73% of their customers, meaning that 27% of their customers are churning. In other words, the churn rate is 27%.

You can look at the percentage using the `normalize = True` input.

## Churn Rate
"""

df_full_train.churn.value_counts(normalize = True) # Displays the percentage
# of each binary input's occurence in the churn column

global_churn_rate = df_full_train.churn.mean() # Another way to get the churn 
# rate is by computing the mean; stored as global_churn_rate.
global_churn_rate # Display global churn rate

round(global_churn_rate, 2) # The global churn rate rounded to 2 decimal places

"""## Numerical & Categorical Variables"""

df_full_train.dtypes # Display the datatypes in df_full_train

"""The three variables in which we are interested are the columns: tenure, monthlycharges, and totalcharges. Not the seniorcitizen, because that is their age.

### Numerical Variables
"""

numerical = ['tenure', 'monthlycharges', 'totalcharges'] # Extract the numerical
# variables from df_full_train; store as numerical

"""### Categorical Variables

To get categorical variables, we can look at all columns; and simply remove the numerical ones; in this case: tenure, monthlycharges, and totalcharges.
"""

df_full_train.columns # Display df_full_train columns

categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',
       'phoneservice', 'multiplelines', 'internetservice',
       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',
       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',
       'paymentmethod']

# Establish the categorical variables.

"""Next, take a look at all the unique values for the categorical variables."""

df_full_train[categorical].nunique() # Calculates the number of unique values
# in each column of the categorical variables that exist in df_full_train.



"""# **Section 3.5: Feature Importance: Churn Rate and Risk Ratio**

[Section 3.5 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/05-risk.md)

Feature importance analysis (part of EDA) - identifying which features affect our target variable.

*   **Churn rate:** Difference between mean of the target variable and mean of categories for a feature. If this difference is greater than 0, it means that the category is less likely to churn, and if the difference is lower than 0, the group is more likely to churn. The larger differences are indicators that a variable is more important than others.
*   **Risk ratio:** Ratio between mean of categories for a feature and mean of the target variable. If this ratio is greater than 1, the category is more likely to churn, and if the ratio is lower than 1, the category is less likely to churn. It expresses the feature importance in relative terms.
*   Mutual information - later

**Functions and methods:**

*   `df.groupby('x').y.agg([mean()])` - returns a dataframe with mean of y series grouped by x series
*   `display(x)` displays an output in the cell of a jupyter notebook.

## Churn Rate

We can look at the churn rate for each group.
"""

df_full_train.head() # Display the first five rows of df_full_train

"""### Female Churn Rate"""

churn_female = df_full_train[df_full_train.gender == 'female'].churn.mean()
# Calculates the churn rate for females in df_full_train; stored as churn_female
churn_female # Display

"""### Male Churn Rate"""

churn_male = df_full_train[df_full_train.gender == 'male'].churn.mean()
# Calculates the churn rate for females in df_full_train; stored as churn_male
churn_male # Display

"""### Compare Female and Male Churn Rate"""

print(f'The global churn rate for is: {round(global_churn_rate, 5) * 100}%.')
print(f'The churn rate for women is: {round(churn_female, 5) * 100}%.')
print(f'The churn rate for men is: {round(churn_male, 5) * 100}%.')

print(f'The difference between the global churn rate and the churn rate for women is: {round(global_churn_rate - churn_female, 5) * 100}%.')
print(f'The difference between the global churn rate and the churn rate for men is: {round(global_churn_rate - churn_male, 5) * 100}%.')

"""## Partner Churn Rate"""

df_full_train.partner.value_counts() #  Calculates the count for how many
# customers live with or without partners

churn_partner = df_full_train[df_full_train.partner == 'yes'].churn.mean()
# Calculates the churn rate for partnered customers in df_full_train; stored 
# as churn_partner
churn_partner # Display

churn_no_partner = df_full_train[df_full_train.partner == 'no'].churn.mean()
# Calculates the churn rate for nonpartnered customers in df_full_train; stored 
# as churn_no_partner
churn_male # Display

"""### Compare Partners vs. No Partners Churn Rates"""

print(f'The global churn rate for is: {round(global_churn_rate, 5) * 100}%.')
print(f'The churn rate for partnered customers is: {round(churn_partner, 5) * 100}%.')
print(f'The churn rate for non partnered customers is: {round(churn_no_partner, 5) * 100}%.')
print(f'The difference between the global churn rate and the churn rate for partnered households is: {round(global_churn_rate - churn_partner, 5) * 100}%.')
print(f'The difference between the global churn rate and the churn rate for non partnered households is: {round(global_churn_rate - churn_no_partner, 5) * 100}%.')

"""There is a noticeable statistical significance between partnered households and non partnered households. This means that with gender, the churn rate does not really matter.

## Feature Importance Introduced

This leads to talking about feature importance.

Difference of Global - Group; if the result is less than 0, then the group is less likely to churn; if the result is more than 0, then the group is more likely to churn.
"""

print(f'The global churn rate for is: {round(global_churn_rate, 5) * 100}%.')
print(f'The churn rate for women is: {round(churn_female, 5) * 100}%.')
print(f'The churn rate for men is: {round(churn_male, 5) * 100}%.')

print(f'The difference between the global churn rate and the churn rate for women is: {round(global_churn_rate - churn_female, 5) * 100}%.')
print(f'The difference between the global churn rate and the churn rate for men is: {round(global_churn_rate - churn_male, 5) * 100}%.')

print(f'The global churn rate for is: {round(global_churn_rate, 5) * 100}%.')
print(f'The churn rate for partnered customers is: {round(churn_partner, 5) * 100}%.')
print(f'The churn rate for non partnered customers is: {round(churn_no_partner, 5) * 100}%.')
print(f'The difference between the global churn rate and the churn rate for partnered households is: {round(global_churn_rate - churn_partner, 5) * 100}%.')
print(f'The difference between the global churn rate and the churn rate for non partnered households is: {round(global_churn_rate - churn_no_partner, 5) * 100}%.')



"""## Risk Ratio

As opposed to taking the difference to see which group is more likely, it is prudent to get percentages to find out which groups are more likely to churn.

Since the men and women saw negligible difference in their churn rate, focus on the partnered and non partnered groups.

The risk ratio is calculated by taking the Group Churn Rate and dividing it by the Global Churn Rate.

`group_churn / global_churn_rate`

If the ratio is greater than one, then the group is more likely to churn; if the ratio is less likely than one, then the group is less likely to churn.
"""

churn_no_partner / global_churn_rate

"""This is to say that customers without partners are 22% more likely to churn."""

churn_partner / global_churn_rate

"""This is to say that customers with a partner are 24% less likely to churn.

## Converting a SQL Query to Pandas

Consider this SQL query,

```
SELECT
  gender,
  AVG(churn),
  AVG(churn) - global_churn AS diff,
  AVG(churn) / global_churn AS risk
FROM
  data
GROUP BY
  gender;
```
we are going to take this query, and convert it to Pandas.
"""

df_full_train.groupby('gender').churn.mean() # Displays the mean (read as: churn
# rate by gender

"""The churn rate by gender does not actually include the other data points required. The code needs to be more inclusive to get the difference and the risk factor using the `agg` method."""

df_group = df_full_train.groupby('gender').churn.agg(['mean', 'count']) # Displays the
# mean (read as: churn rate) but includes the count; stored as df_group.
df_group # Display

df_group['diff'] = df_group['mean'] - global_churn_rate # Generates a new column
# in the above table to include the difference
df_group['risk'] = df_group['mean'] / global_churn_rate # Generates a new column
# in the above table to include the risk ratio
df_group # Display

"""## Using a For Loop to Display Churn Info for all the Categorical Variables

We composed this table for gender and household partnerships; now we include all of the categories from the previous section using a for loop.
"""

from IPython.display import display # This allows us to display the for loop
# results.

for c in categorical:
  print(c) # This will print the variable name above each respective table
  df_group = df_full_train.groupby(c).churn.agg(['mean', 'count'])
  df_group['diff'] = df_group['mean'] - global_churn_rate # Generates a new column
  # in the above table to include the difference
  df_group['risk'] = df_group['mean'] / global_churn_rate # Generates a new column
  # in the above table to include the risk ratio
  display(df_group) # Display
  print() # Generates a line break.
  print() # Generates a line break.



"""# **Section 3.6: Features Importance: Mutual Information**

[Section 3.6 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/06-mutual-info.md)

Mutual information - concept from information theory, it tells us how much we can learn about one variable if we know the value of another.

https://en.wikipedia.org/wiki/Mutual_information

**Notes:**

Mutual information is a concept from information theory, which measures how much we can learn about one variable if we know the value of another. In this project, we can think of this as how much do we learn about churn if we have the information from a particular feature. So, it is a measure of the importance of a categorical variable.

Classes, functions, and methods:

*   `mutual_info_score(x, y)` - Scikit-Learn class for calculating the mutual information between the x target variable and y feature.
*   `df[x].apply(y)` - apply a y function to the x series of the df dataframe.
*   `df.sort_values(ascending=False).to_frame(name='x')` - sort values in an ascending order and called the column as x.

## Import and Implement Mutual Information Score from Scikit-Learn
"""

from sklearn.metrics import mutual_info_score # Imports mutual_info_score from
# the Scikit-Learn library

"""This outputs a value that conveys how much we can learn about the contract variable by observing the churn variable; and vice-versa."""

mutual_info_score(df_full_train.churn, df_full_train.contract)

mutual_info_score(df_full_train.contract, df_full_train.churn)

"""The operation is commutative; it does not matter in which sequence you communicate the the two variables."""

mutual_info_score(df_full_train.churn, df_full_train.gender)

"""When comparing trying to understand how much we can learn about the churn by observing gender, we see that this rate is low."""

mutual_info_score(df_full_train.churn, df_full_train.partner)

"""Partner is more important than gender, but far less important than contract.

Now, we are going to use the `apply` command in order to assess the mutual information with the rest of the categorical variables.

We first need to wrap it inside of a function.
"""

def mutual_info_churn_score(series):
  return mutual_info_score(series, df_full_train.churn)

"""We are also going to sort the data so that the most important information comes first.


"""

mi = df_full_train[categorical].apply(mutual_info_churn_score) # Outputs the
# mutual information scores for all of the categorical variables in the
# df_full_train; stores as mi (short for mutual information)
mi.sort_values(ascending = False) # Displays in descending order



"""# **Section 3.7: Feature Importance: Correlation**

[Section 3.7 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/07-correlation.md)

How about numerical columns?

Correlation coefficient - https://en.wikipedia.org/wiki/Pearson_correlation_coefficient

**Notes:**

Correlation coefficient measures the degree of dependency between two variables. This value is negative if one variable grows while the other decreases, and it is positive if both variables increase. Depending on its size, the dependency between both variables could be low, moderate, or strong. It allows measuring the importance of numerical variables.

Correlation coefficients exist on a range: [-1, +1]. The question as to their magnitude is contingent on which 

Two variables have a *low correlation* if their correlations coefficient exists in the range: +/- [0.0, 0.2]. This means that the two variables rarely impact each other.

Two variables have a *moderate correlation* if their correlation coefficient exists in the range: +/- (0.2,0.6]. This means that the two variables sometimes impact each other.

Two variables have a *strong correlation* if their correlation coefficient exists in the range: +/- (0.6, 1.0]. This means that the two variables often impact each other. When the resulting correlation coefficent is greater than 0.9, then the two variables almost always impact each other.

The target variable, y, is going to exist in the range {0,1}, and X can be any real number.

**Functions and methods:**

*   `df[x].corrwith(y)` - returns the correlation between x and y series.
"""

df_full_train.tenure.max()

"""So, the max value for the tenure column is 72; so the variable X exists within the domain [0, 72].

This means the y-value can only take on terms in the range [0, 1].

A positive correlation means that the more X increase, the more tenure, correlates to a higher churn rate.

A negative correlation means that the more X increase, the less tenure, correlates to a lower churn rate.

Zero correlation means that the variable does not affect the churn rate at all.
"""

df_full_train[numerical] # Generates the columns with numerical datatypes from
# df_full_train

"""We want calculate the corrlation of these numerical columns with the churn column."""

df_full_train[numerical].corrwith(df_full_train.churn) # Generates the
# correlation coefficient between the numerical variables in the dataset, with
# the churn variables in the dataset.

"""Displayed are the correlation coefficients for each of the numerical variables.

For `tenure`, it is being observed that there is a moderately negative correlation between `tenure` and `churn`. The longer a customer stays with the company, the less likely they are to churn.

For `monthlycharges`, it is being observed that there is a low positive correlation between `monthlycharges` and `churn`. The more people pay for monthly charges, the more likely they are to churn.

For `totalcharges`, it is being observed that there is a low negative correlation between `monthlycharges` and `churn`. The more people pay in total, the less likely they are to churn, which sounds counterintuitive.

The latter two numerical variables actually appear to have complete opposite corrlations with `churn`.

## Correlation Coefficient between Tenure and Churn
"""

df_full_train[df_full_train.tenure <= 2].churn.mean() # Assessing the churn rate
# for customers who spent 2 months or less with the company.

df_full_train[df_full_train.tenure > 2].churn.mean() # Assessing the churn rate
# for customers who spent more than 2 months with the company.

df_full_train[(df_full_train.tenure > 2) & (df_full_train.tenure <= 12)].churn.mean() 
# Assessing the churn rate for customers who spent more than 2 months and less
# than or equal to 12 months with the company.

df_full_train[df_full_train.tenure > 12].churn.mean() # Assessing the churn rate
# for customers who spent more than 12 months with the company.

"""For `tenure`, there exists a negative correlation with `churn`.

## Correlation Coefficient between Monthly Charges and Churn
"""

df_full_train[df_full_train.monthlycharges <= 20].churn.mean() # Assessing the 
# churn rate for customers who spent $20 or less per month.

df_full_train[(df_full_train.monthlycharges > 20) & (df_full_train.monthlycharges <= 50)].churn.mean() 
# Assessing the churn rate for customers who spent more than $20 and $ 50 or 
# less per month.

df_full_train[df_full_train.monthlycharges > 50].churn.mean() # Assessing the 
# churn rate for customers who spent more than $50 per month.

"""For `monthlycharges`, there exists a positve correlation with `churn`.

# **Section 3.8: One-Hot Encoding**

[Section 3.8 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/08-ohe.md)

*   Use Scikit-Learn to encode categorical features

**Notes:**

One-Hot Encoding allows encoding categorical variables in numerical ones. This method represents each category of a variable as one column, and a 1 is assigned if the value belongs to the category or 0 otherwise.

**Classes, functions, and methods:**

*   `df[x].to_dict(oriented='records')` - convert x series to dictionaries, oriented by rows.
*   `DictVectorizer().fit_transform(x)` - Scikit-Learn class for converting x dictionaries into a sparse matrix, and in this way doing the one-hot encoding. It does not affect the numerical variables.
*   `DictVectorizer().get_feature_names()` - returns the names of the columns in the sparse matrix.

## Import DictVectorizer from Scikit-Learn
"""

from sklearn.feature_extraction import DictVectorizer

df_train[['gender', 'contract']].iloc[:10] # Display the first 10 rows of the
# gender and contract categorical variables from the training dataframe

"""Initially, the instructor used `iloc[:10]`, but we ended up generating a 4-column matrix down below, when we required a 5-column matrix. The first 10 rows had no instances of a 1-year contract, so it was changed to 100."""

dicts = df_train[['gender', 'contract']].iloc[:100].to_dict(orient = 'records') 
# Turns these data elements from the first 100 rows into a dictionary; 
# stored as dicts

dv = DictVectorizer(sparse = False)

dv.fit(dicts)

dv.get_feature_names()

"""These feature names correspond to the matrix below; the month-to-month contract correspeonds to the first column, the male customers respond to the far right column.

The first three columns are for contracts, and the last two columns are for gender.
"""

dv.transform(dicts)

"""Suppose we have a dataframe with two categorical variables and one numerical variable."""

dicts = df_train[['gender', 'contract', 'tenure']].iloc[:100].to_dict(orient = 'records') 
# Turns these data elements into a dictionary; stored as dicts. This version of
# dicts has tenure added.

dicts[0]

dv = DictVectorizer(sparse = False)

dv.fit(dicts)

dv.get_feature_names()

dv.transform(dicts)

"""The numerical variable remains unchanged; as seen in the far right column. Tenure was added on as the far right column.

## DictVectorizer and the Training Dataframe
"""

train_dicts = df_train[categorical + numerical].to_dict(orient = 'records')

train_dicts[0]

dv = DictVectorizer(sparse = False)

dv.fit(train_dicts)

dv.get_feature_names()

list(dv.transform(train_dicts[:5])[0]) # Shows the first row as a list

"""## DictVectorizer Final Walkthrough

### Training Dataset
"""

train_dicts = df_train[categorical + numerical].to_dict(orient = 'records')

dv = DictVectorizer(sparse = False)

dv.fit(train_dicts)
X_train = dv.transform(train_dicts)

X_train.shape

X_train = dv.fit_transform(train_dicts) # Shorter way of writing line 218

X_train.shape

"""### Validation Dataset"""

val_dicts = df_val[categorical + numerical].to_dict(orient = 'records')

dv = DictVectorizer(sparse = False)

dv.fit(val_dicts)
X_val = dv.transform(val_dicts)

X_val = dv.transform(val_dicts)

X_val.shape



"""# **Section 3.9: Logistic Regression**

[Section 3.9 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/09-logistic-regression.md)

*   Binary classification
*   Linear vs logistic regression
"""

dv = DictVectorizer(sparse = False)

train_dict = df_train[categorical + numerical].to_dict(orient = 'records')
X_train = dv.fit_transform(train_dict)

val_dict = df_val[categorical + numerical].to_dict(orient = 'records')
X_val = dv.transform(val_dict)



"""Recall:

g(x_i) = y_i; where g is the model, x_i is the individual data point, y_i is the target for that individual data point.

The value, y_i can be either a regression or a classification; and classification can be either binary or multiclass. While there are other types of classifications, this will be enough for now.

The binary classification will be the focus because this is the problem being solved by logistic regression.

So, for binary classification, y_i exists in a set {0, 1}, where y_i can be either 0 or 1 (0 being negative and 1 being positive; where not churning is negative, and churning is positive. Another example can be the not spam versus spam example.

Now consider g(x_i); which outputs a number between 0 and 1, a number which is treated as a probability of x_i belonging to the positive class.

Recall, the formul for linear regression is:

g(x_i) = w_0 + (w^T)x_i; the output of which can be any number from negative infinity to positive infinity, that is all real numbers. The term w_0 is the biased term, w is the weights vector - in this case transposed, and is multiplied by the features matrix.

Logistic regression works in a similar spirit. It has a similar expression, there's the bias term and the weights. However, the distinction lies in that in outputs a number between 0 and 1. It is doing this by using a special function known as **sigmoid**.

So, it takes the expression, and runs it through the sigmoid, and it generates an output between 0 and 1.

## Sigmoid Example
"""

def sigmoid(z):
  return 1 / (1 + np.exp(-z))
  
# Variable z is the sigmoid input, and iscomprised of the terms
# that you observed in the linear regression equation.
# The return is the program syntax for the formula of a sigmoid.

z = np.linspace(-7, 7, 51) # Generates an array of 51 values that exist between
# -5 and +5; stored as z
z # Display

sigmoid(z) # Plug z into the sigmoid function

pyplot.plot(z, sigmoid(z)) # Plot the sigmoid with respect to z.

"""So, in effect, the input z is a score generated from the linear regression expression; and the sigmoid generates a probability based on the input z."""

def linear_regression(x_i):
  result = w_0

  for j in range(len(w)):
    result = result + x_i[j] * w[j]

  return result

def logistic_regression(x_i):
  score = w_0

  for j in range(len(w)):
    score = score + x_i[j] * w[j]

  result = sigmoid(score)
  return result

"""# **Section 3.10: Training Logistic Regression with Scikit-Learn**

[Section 3.10 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/10-training-log-reg.md)

*   Train a model with Scikit-Learn
*   Apply it to the validation dataset
*   Calculate the accuracy

**Notes:**

This video was about training a logistic regression model with Scikit-Learn, applying it to the validation dataset, and calculating its accuracy.

**Classes, functions, and methods:**

*   `LogisticRegression().fit_transform(x)` - Scikit-Learn class for calculating the logistic regression model.
*   `LogisticRegression().coef_[0]` - returns
the coeffcients or weights of the LR model
*   `LogisticRegression().intercept_[0]` - returns the bias or intercept of the LR model
*   `LogisticRegression().predict[x]` - make predictions on the x dataset
*   `LogisticRegression().predict_proba[x]` - make predictions on the x dataset, and returns two columns with their probabilities for the two categories - soft predictions

## Import and Implement Logistic Regression Package
"""

from sklearn.linear_model import LogisticRegression # Import the Logistic
# Regression package from Scikit-Learn

model = LogisticRegression()
model.fit(X_train, y_train)

model.intercept_[0] # Displays the biased term; bias and intercept are synonymous.

model.coef_[0].round(3) # Displays the weights vector for the linear regression

model.predict(X_train) # These are called hard predictions; 1 is churn, 0 is
# not churn. This does not output the probability

y_pred = model.predict_proba(X_train) # These are soft predictions; generates probability
# outputs; stored as y_pred.

"""There is a reason for 2 columns; the column on the left is the probability of there being a 0 on the binary output, and the column on the right is the probability of it being 1 on the binary output.

The probability for churning is what we are interested in assessing; in other words, the right column.

Extract the right column.
"""

y_pred = model.predict_proba(X_val)[:, 1] # Predicts the probability for churn
# while extracting the right column - the column affirming the customer will
# churn; executed on the validation dataset; stored as y_pred

churn_decision = y_pred >= 0.5 # Generates a binary array based on which 
# customers are likely to churn

df_val[churn_decision].customerid # The company could consider sending
# promotional material to these customers based on the prediction

"""## Verifying Prediction Accuracy"""

y_val # Display

churn_decision.astype(int) # Convert the datatype in churn_decision from binary
# to an integer.

"""We can see some commonalities between y validation and the churn decision."""

(y_val == churn_decision).mean() # Assess how many values match between both
# datasets and calculates how many of them actually match

"""Approximately 80% of the validation dataset shares common elements with the churn decision.

Next, let's take a look at what is happening inside line 134.
"""

df_pred = pd.DataFrame() # Create dataframe to see what is occuring inside;
# stored as df_pred
df_pred['probability'] = y_pred # This is the soft predictions; the probability
# the customer will churn
df_pred['prediction'] = churn_decision.astype(int) # The churn prediction; the
# data types converted to integers
df_pred['actual'] = y_val # The actual value

df_pred

"""Next, we need to see how many of them are correct."""

df_pred['correct'] = df_pred.prediction == df_pred.actual # Operation to see
# how many of the predictions are correct.
df_pred # Display

df_pred.correct.mean() # Display the percentage that is correct

"""Line 134 is the shortcut to performing the entire operation we just completed. Our model is 80% correct.

# **Section 3.11: Model Interpretation**

[Section 3.11 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/11-log-reg-interpretation.md)

*   Look at the coefficients
*   Train a smaller model with fewer features

**Notes:**

This video was about the interpretation of coefficients, and training a model with fewer features.

In the formula of the logistic regression model, only one of the one-hot encoded categories is multiplied by 1, and the other by 0. In this way, we only consider the appropriate category for each categorical feature.

**Classes, functions, and methods:**

*   `zip(x,y) `- returns a new list with elements from x joined with their corresponding elements on y
"""

dv.get_feature_names() # Gets the feature names

model.coef_[0].round(3) # Copied from line 121

"""Now, we are going to join the names of the features and join them with the coefficients, we will see what is the weight for each feature.

This is done with the zip function. What zip does

## Zip It Up

Just to show what the zip feature does on a simple example, consider the following.
"""

a = [1, 2, 3, 4] # Small sample array
b = 'abcd' # Small sample string

list(zip(a, b)) # Generates a list where the first element of a is joined with
# the first element of b, and so forth.

dict(zip(a, b)) # Generates a dictionary where all the elements from a become
# keys, and all the elements from b become values.

"""### Zipping the Features and Model Coefficients"""

dict(zip(dv.get_feature_names(), model.coef_[0].round(3))) # Generates a
# dictionary where the feature names are the keys, and the model coefficients
# are values in order to see the values for each of the variables.

"""The list above is cumbersome to process, so we can actually just train a smaller model.

We can take a subset of features.
"""

small = ['contract', 'tenure', 'monthlycharges'] # Smaller array with less
# features

df_train[small].iloc[:10].to_dict(orient = 'records') # This will be used for
# the vectorizer

dicts_train_small = df_train[small].to_dict(orient = 'records')
dicts_val_small = df_val[small].to_dict(orient = 'records')

dv_small = DictVectorizer(sparse = False)
dv_small.fit(dicts_train_small)

dv_small.get_feature_names()

X_train_small = dv_small.transform(dicts_train_small)

model_small = LogisticRegression()
model_small.fit(X_train_small, y_train)

w_0 = model_small.intercept_[0] # Bias term
w_0 # Display

w = model_small.coef_[0]
w.round(3)

dict(zip(dv_small.get_feature_names(), w.round(3))) # These are the weights

sigmoid(-2.47) # Inserting the biased term into the sigmoid generates a result
# that says without knowing much about the customer, their probability of
# churning over is low.

-2.47 + 0.97 + 50 * 0.027 + 5 * (-0.036)

# We want this to be our score - the input into our sigmoid.

# Add the 0.97, for their month-to-month
# contract is 0.97 * 1 (since 1 is true for monthly contract).
# When we learn that this customer has a monthly contract, we see a significant 
# increase. Just this increases to 0.18242552380635632.

# We also know the customer pays $50/month, and that for every $1 more they pay
# the probability increases when you multiply it by 0.027. This increases to
# 0.46257015465625034

# Adding the tenure; 5 months with the company - multiplied by the weight -
# decreases the sigmoid output to 0.41824062315816374

sigmoid(_) # The underscore acts as a stand-in for the previous cell's output
# and put it as the parameter.

"""# **Section 3.12: Using the Model**

[Section 3.12 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/12-using-log-reg.md)

**Notes:**

We trained the logistic regression model with the full training dataset (training + validation), considering numerical and categorical features. Thus, predictions were made on the test dataset, and we evaluate the model using the accuracy metric.

In this case, the predictions of validation and test were similar, which means that the model is working well.
"""

dicts_full_train = df_full_train[categorical + numerical].to_dict(orient = 'records')

dicts_full_train[:3] # First 3 rows of the previous line

dv = DictVectorizer(sparse = False)
X_full_train = dv.fit_transform(dicts_full_train)

y_full_train = df_full_train.churn.values

model = LogisticRegression()
model.fit(X_full_train, y_full_train)

dicts_test = df_test[categorical + numerical].to_dict(orient = 'records')

X_test = dv.transform(dicts_test)

y_pred = model.predict_proba(X_test)[:, 1]

churn_decision = (y_pred >= 0.5)

(churn_decision == y_test).mean() # Slightly more accurate

"""## Single Customer"""

customer = dicts_test[10] # Customer 10
customer

X_small = dv.transform([customer])

X_small.shape

model.predict_proba(X_small)

model.predict_proba(X_small)[0, 1]

y_test[10]



"""# **Section 3.13: Summary**

[Section 3.13 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/13-summary.md)

*   Feature importance - risk, mutual information, correlation
*   One-hot encoding can be implemented with `DictVectorizer`
*   Logistic regression - linear model like linear regression
*   Output of log reg - probability
*   Interpretation of weights is similar to linear regression

In this session, we worked on a project to predict churning in customers from a company. We learned the feature importance of numerical and categorical variables, including risk ratio, mutual information, and correlation coefficient. Also, we understood one-hot encoding and implemented logistic regression with Scikit-Learn.
"""



"""# **Section 3.14: Explore More**

[Section 3.14 Instructor Page](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/03-classification/14-explore-more.md)

More things

*   Try to exclude least useful features

Use scikit-learn in project of last week

*   Re-implement train/val/test split using scikit-learn in the project from the last week
*   Also, instead of our own linear regression, use `LinearRegression` (not regularized) and `RidgeRegression` (regularized). Find the best regularization parameter for Ridge

Other projects

*   Lead scoring - https://www.kaggle.com/ashydv/leads-dataset
*   Default prediction - https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients
"""