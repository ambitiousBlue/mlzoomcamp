# -*- coding: utf-8 -*-
"""Adkins_J_HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kpwEDfvAhNcTqxHBH6ZDlOSbqyVN4GPD

# **Import Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries
import pandas as pd # Import Pandas library
import numpy as np # Import NumPy library

import seaborn as sns # Import Seaborn
from matplotlib import pyplot as plt # Import Plotting tools
# %matplotlib inline

"""# **Features**

For the rest of the homework, you'll need to use only these columns:


*   `'latitude'`,
*   `'longitude'`,
*   `'housing_median_age'`,
*   `'total_rooms'`,
*   `'total_bedrooms'`,
*   `'population'`,
*   `'households'`,
*   `'median_income'`,
*   `'median_house_value'`,
*   `'ocean_proximity'`

# **Dataset**

## **Import Dataset**

In this homework, we will use the California Housing Prices data from [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).

Here's a wget-able [link](https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv).

We'll keep working with the `'median_house_value'` variable, and we'll transform it to a classification task.
"""

# Input the data from the URL
data = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'

!wget $data # Get the data

"""## **Data Preparation & Exploratory Data Analysis**

*   Select only the features from above and fill in the missing values with 0.
"""

df = pd.read_csv('housing.csv') # Load the data using the read_csv command; store
# as a dataframe. Make sure to input the correct file name in the argument.

df.head() # View first 5 rows of the dataframe.

df.shape # Size of the dataset.

df.isnull().sum() # Display null values

df = df.fillna(0) # Fill null values with 0

df.isnull().sum() # Display null values

"""*   Create a new column `rooms_per_household` by dividing the column `total_rooms` by the column `households` from dataframe.
*   Create a new column `bedrooms_per_room` by dividing the column `total_bedrooms` by the column `total_rooms` from dataframe.
*   Create a new column `population_per_household` by dividing the column `population` by the column `households` from dataframe.
"""

df = df.copy()
df['room_per_household'] = df['total_rooms'] / df['households']
df['bedrooms_per_household'] = df['total_bedrooms'] / df['total_rooms']
df['population_per_household'] = df['population'] / df['households']

df.shape # Size of the dataset.

df.head() # View first 5 rows of the dataframe.

"""# **Question 1: Frequent Observation (Mode)**

What is the most frequent observation (mode) for the column `ocean_proximity`?

Options

*   `NEAR BAY`
*   `<1H OCEAN`
*   `INLAND`
*   `NEAR OCEAN`
"""

ocean_prox_mode = df['ocean_proximity'].mode() # Mode for ocean_proximity

"""## Question 1 Solution"""

ocean_prox_mode # Display

"""# **Question 2: Correlations**"""

from sklearn.model_selection import train_test_split

"""*   Create the [correlation matrix](https://www.google.com/search?q=correlation+matrix) for the numerical features of your train dataset.
 *   In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset.
*   What are the two features that have the biggest correlation in this dataset?

Options:

*   `total_bedrooms` and `households`
*   `total_bedrooms` and `total_rooms`
*   `population` and `households`
*   `population_per_household` and `total_rooms`

## Split the Data

*   Split your data in train/val/test sets, with 60%/20%/20% distribution.
*   Use Scikit-Learn for that (the `train_test_split` function) and set the seed to 42.
*   Make sure that the target value (`median_house_value`) is not in your dataframe.
"""

df_full_train, df_test = train_test_split(df, test_size = 0.2, random_state = 42)
# Split dataframe between full training data (training + validation) and test
# data; set as df_full_train and df_test

df_train, df_val = train_test_split(df_full_train, test_size = 0.25, random_state = 42)
# Split df_full_train into df_train and df_val with the same parameters

len(df) # Display full length of dataframe

len(df_train), len(df_val), len(df_test) # Display length of split dataframes

len(df_train) + len(df_val) + len(df_test) # The lengths of each split dataframe
# sum up to the length of the entire dataframe.

"""## Make `median_house_value` Binary

*   We need to turn the `median_house_value` variable from numeric into binary.
*   Let's create a variable above_average which is `1` if the `median_house_value` is above its mean value and `0` otherwise.
"""

mean_house_value = df_full_train.median_house_value.mean()
mean_house_value # Display

df_full_train['above_average'] = df_full_train.median_house_value >= mean_house_value
# Creates variable 'above_average' in the full training dataframe

df_full_train.above_average = (df_full_train.above_average == True).astype(int) 
# Converts boolean datatype to integer

df_full_train.head() # View first 5 rows of the full training dataframe.

df_full_train.above_average.value_counts(normalize = True)
# Displays the percentage of true and false in above_average column

"""## Question 2 Solution"""

df_full_train.corr() # Correlation scores for df_full_train columns

"""Highest correlation rate is between:

`'households'` and `'total_bedrooms'` at 0.980255

# **Question 3: Mutual Information Scores**

*   Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.
*   What is the value of mutual information?
*   Round it to 2 decimal digits using `round(score, 2)`

Options:

*   0.26
*   0
*   0.10
*   0.16
"""

from sklearn.metrics import mutual_info_score # Imports mutual_info_score from
# the Scikit-Learn library

"""## Question 3 Solution"""

mutual_info_score(df_full_train.ocean_proximity, df_full_train.above_average)
# Mutual info score for training dataframe

"""# **Question 4: Training Logistic Regression and Calculating the Validation Dataset's Accuracy**

*   Now let's train a logistic regression
*   Remember that we have one categorical variable `'ocean_proximity'` in the data. Include it using one-hot encoding.
*   Fit the model on the training dataset.
 *   To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:
 *   `model = LogisticRegression(solver="liblinear", C=1.0, max_iter=1000, random_state=42)`
*   Calculate the accuracy on the validation dataset and round it to 2 decimal digits.

Options:

*   0.60
*   0.72
*   0.84
*   0.95
"""

mean_house_value = df.median_house_value.mean()
mean_house_value # Display

df['above_average'] = df.median_house_value >= mean_house_value

df['above_average'] = (df.above_average == True).astype(int)

df_full_train, df_test = train_test_split(df, test_size = 0.2, random_state = 42)
# Split the dataframe to df_full_train and df_test

df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state = 42)
# Split df_full_train to df_train and df_val

y_train = df_train.above_average.values
y_val = df_val.above_average.values
y_test = df_test.above_average.values
del df_train['above_average']
del df_val['above_average']
del df_test['above_average']

del df_train['median_house_value']
del df_val['median_house_value']
del df_test['median_house_value']

train_dict = df_train.to_dict(orient='records')
train_dict[0]

from sklearn.feature_extraction import DictVectorizer

dv = DictVectorizer(sparse = False)

X_train = dv.fit_transform(train_dict)

val_dict = df_val.to_dict(orient = 'records')
X_val = dv.transform(val_dict)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver = "liblinear", C = 1.0, max_iter = 1000, random_state = 42)
model.fit(X_train, y_train)

w = model.coef_[0].round(3) # Weighted coefficients

w_0 = model.intercept_[0] # Bias term
w_0

y_pred = model.predict_proba(X_val)[:, 1]
y_pred # Display

above_average = (y_pred >= 0.5)

df_val[above_average]

"""## Question 4 Solution"""

(y_val == above_average).mean()

"""# **Question 5: Feature Elimination**

*   Let's find the least useful feature using the feature elimination technique.
*   Train a model with all these features (using the same parameters as in Q4).
*   Now exclude each feature from this set and train a model without it. Record the accuracy for each model.
*   For each feature, calculate the difference between the original accuracy and the accuracy without the feature.
*   Which of following feature has the smallest difference?
 *   `total_rooms`
 *   `total_bedrooms`
 *   `population`
 *   `households`

Note: the difference doesn't have to be positive
"""

above_average

y_val

above_average.astype(int)

df_pred = pd.DataFrame()
df_pred['probability'] = y_pred
df_pred['prediction'] = above_average.astype(int)
df_pred['actual'] = y_val
df_pred['correct'] = df_pred.prediction == df_pred.actual

df_pred # Display

dict(zip(dv.get_feature_names(), model.coef_[0].round(3)))

small = ['total_rooms', 'total_bedrooms', 'population', 'households']

dicts_train_small = df_train[small].to_dict(orient = 'records')
dicts_val_small = df_val[small].to_dict(orient = 'records')

dv_small = DictVectorizer(sparse=False)
dv_small.fit(dicts_train_small)

X_train_small = dv_small.transform(dicts_train_small)

model_small = LogisticRegression(solver="liblinear", C=1.0, max_iter=1000, random_state=42)
model_small.fit(X_train_small, y_train)

w_0 = model_small.intercept_[0] # Bias term
w_0 # Display

w = model_small.coef_[0] # Weights
w # Display

dict(zip(dv_small.get_feature_names_out(), w.round(3)))

X_val_small = dv_small.transform(dicts_val_small)

y_pred = model_small.predict_proba(X_val_small)[:, 1]

above_average = (y_pred >= 0.5)
(y_val == above_average).mean()

"""## Question 5 Solution"""

small = ['total_rooms', 'total_bedrooms', 'population', 'households']
orig = 0.7095445736434108
for i in range(len(small)):
    feature = small.pop()
    dicts_train = df_train[small].to_dict(orient='records')
    dicts_val = df_val[small].to_dict(orient='records')

    dv = DictVectorizer(sparse=False)
    dv.fit(dicts_train)
    X_train = dv.transform(dicts_train)
    X_val = dv.transform(dicts_val)
    model = LogisticRegression(solver="liblinear", C=1.0, max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict_proba(X_val)[:, 1]
    above_average = (y_pred >= 0.5)
    accuracy = (y_val == above_average).mean()
    print(f'Without {feature}, accuracy is {accuracy}; difference between original is {round(orig - accuracy, 3)}')
    small.insert(0, feature)

"""# **Question 6: Linear Regression and the Best RSME**

*   For this question, we'll see how to use a linear regression model from Scikit-Learn
*   We'll need to use the original column `'median_house_value'`. Apply the logarithmic transformation to this column.
*   Fit the Ridge regression model (`model = Ridge(alpha=a, solver="sag", random_state=42)`) on the training data.
*   This model has a parameter `alpha`. Let's try the following values: `[0, 0.01, 0.1, 1, 10]`
*   Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.

If there are multiple options, select the smallest `alpha`.

Options:

*   0
*   0.01
*   0.1
*   1
*   10
"""

df_full_train, df_test = train_test_split(df, test_size = 0.2, random_state = 42)
df_train, df_val = train_test_split(df_full_train, test_size = 0.25, random_state = 42)

y_train = np.log1p(df_train.median_house_value.values)
y_val = np.log1p(df_val.median_house_value.values)
y_test = np.log1p(df_test.median_house_value.values)

del df_train['median_house_value']
del df_val['median_house_value']
del df_test['median_house_value']

from sklearn.linear_model import Ridge

dv_ridge = DictVectorizer(sparse=False)

train_dict = df_train[['ocean_proximity']].to_dict(orient = 'records')
X_train = dv.fit_transform(train_dict)

val_dict = df_val[['ocean_proximity']].to_dict(orient = 'records')
X_val = dv.transform(val_dict)

def rmse(y, y_pred):
    se = (y - y_pred) ** 2
    mse = se.mean()
    return np.sqrt(mse)

"""## Question 6 Solution"""

alphas = [0, 0.01, 0.1, 1, 10]
for a in alphas:
    
    model_ridge = Ridge(alpha=a, solver="sag", random_state=42)
    model_ridge.fit(X_train, y_train)
    
    w0 = model_ridge.intercept_
    w = model_ridge.coef_
    y_pred = w0 + X_val.dot(w)
    print(f"For alpha = {a:>5}; we round to {round(rmse(y_val, y_pred), 4)}")